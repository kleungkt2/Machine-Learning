{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport csv\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.optim import Adam\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2021-11-13T15:50:37.000712Z","iopub.execute_input":"2021-11-13T15:50:37.001522Z","iopub.status.idle":"2021-11-13T15:50:37.007199Z","shell.execute_reply.started":"2021-11-13T15:50:37.001473Z","shell.execute_reply":"2021-11-13T15:50:37.006320Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"FILE_PREFIX = '../input/ml-2021fall-hw3/'\nTRA_PATH = FILE_PREFIX + 'data/train/'\nTST_PATH = FILE_PREFIX + 'data/test/'\nLABEL_PATH = FILE_PREFIX + 'data/train.csv'\nDEVICE_ID = 0\nSEED = 5566\nNUM_ECPOCH = 300\n\ntorch.cuda.set_device(DEVICE_ID)\nuse_gpu = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_gpu else \"cpu\")\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:35:00.081286Z","iopub.execute_input":"2021-11-13T14:35:00.081567Z","iopub.status.idle":"2021-11-13T14:35:00.088191Z","shell.execute_reply.started":"2021-11-13T14:35:00.081535Z","shell.execute_reply":"2021-11-13T14:35:00.087522Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def load_train_data(img_path, label_path, valid_ratio=0.12):\n    train_label = pd.read_csv(label_path)['label'].values.tolist()\n    train_image = [f'{img_path}/{i+7000}.jpg' for i in range(len(train_label))]\n    \n    train_data = list(zip(train_image, train_label))\n    random.shuffle(train_data)\n    \n    split_len = int(len(train_data) * valid_ratio)\n    train_set = train_data[split_len:]\n    valid_set = train_data[:split_len]\n    \n    return train_set, valid_set\n\ndef load_test_data(img_path):\n    test_set = [f'{img_path}/{i}.jpg' for i in range(7000)]\n    return test_set\n    \ndef compute_statistics(dataset):\n    data = []\n    for (img_path, label) in dataset:\n        data.append(np.array(Image.open(img_path)))\n    data = np.array(data)\n    return data.mean(), data.std()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:35:00.620916Z","iopub.execute_input":"2021-11-13T14:35:00.621527Z","iopub.status.idle":"2021-11-13T14:35:00.629117Z","shell.execute_reply.started":"2021-11-13T14:35:00.621492Z","shell.execute_reply":"2021-11-13T14:35:00.628337Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_set, valid_set = load_train_data(TRA_PATH, LABEL_PATH)\ntest_set = load_test_data(TST_PATH)\n\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(), # 隨機將圖片水平翻轉\n    transforms.RandomRotation(15), # 隨機旋轉圖片\n    ])\n#transform = None","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:35:01.854862Z","iopub.execute_input":"2021-11-13T14:35:01.855745Z","iopub.status.idle":"2021-11-13T14:35:01.932718Z","shell.execute_reply.started":"2021-11-13T14:35:01.855692Z","shell.execute_reply":"2021-11-13T14:35:01.932070Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class FaceExpressionDataset(Dataset):\n    def __init__(self, data, augment=None):\n        self.data = data\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.data)\n    \n    def normalize(self, data):\n        # TODO\n        data /= 255.\n        return data\n    \n    def read_img(self, idx):\n        img = Image.open(self.data[idx][0])\n        if not self.augment is None:\n            img = self.augment(img)\n        img = torch.from_numpy(np.array(img)).float()\n        img = img.unsqueeze(0).float()\n        img = self.normalize(img)\n        return img\n    \n    def __getitem__(self, idx):\n        img = self.read_img(idx)\n        label = self.data[idx][1]\n        return img, label\n    \nclass TestingDataset(Dataset):\n    def __init__(self, data, augment=None):\n        self.data = data\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.data)\n    \n    def normalize(self, data):\n        # TODO\n        data /= 255.\n        return data\n    \n    def read_img(self, idx):\n        img = Image.open(self.data[idx])\n        if not self.augment is None:\n            img = self.augment(img)\n        img = torch.from_numpy(np.array(img)).float()\n        img = img.unsqueeze(0).float()\n        img = self.normalize(img)\n        return img\n        \n    def __getitem__(self, idx):\n        img = self.read_img(idx)\n        return img","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:35:04.183987Z","iopub.execute_input":"2021-11-13T14:35:04.184703Z","iopub.status.idle":"2021-11-13T14:35:04.198459Z","shell.execute_reply.started":"2021-11-13T14:35:04.184666Z","shell.execute_reply":"2021-11-13T14:35:04.197250Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dataset = FaceExpressionDataset(train_set, transform)\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n\nvalid_dataset = FaceExpressionDataset(valid_set)\nvalid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n\ntest_dataset = TestingDataset(test_set)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)                  ","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:35:09.094578Z","iopub.execute_input":"2021-11-13T14:35:09.094844Z","iopub.status.idle":"2021-11-13T14:35:09.099809Z","shell.execute_reply.started":"2021-11-13T14:35:09.094809Z","shell.execute_reply":"2021-11-13T14:35:09.099158Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def gaussian_weights_init(m):\n    classname = m.__class__.__name__\n    # 字符串查找find，找不到返回-1，不等-1即字符串中含有该字符\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.04)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:35:12.402544Z","iopub.execute_input":"2021-11-13T14:35:12.403471Z","iopub.status.idle":"2021-11-13T14:35:12.407943Z","shell.execute_reply.started":"2021-11-13T14:35:12.403432Z","shell.execute_reply":"2021-11-13T14:35:12.407011Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#Formula for calculating N * N\n# N = (W-F+2P)/S + 1\n# W = input, F = kernel, S = Stride, P = Padding\nclass FaceExpressionNet(nn.Module):\n    def __init__(self, num_classes: int = 7, dropout: float = 0.3) -> None:\n        super().__init__()\n        #input:(128, 1, 64, 64)\n        #output:(128, 64, 32, 32)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #(64-5+2*2)/1+1 = 64\n            nn.BatchNorm2d(64),\n            nn.RReLU(inplace = True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding = 0), #(64-2+2*0)/2+1 = 32\n            nn.Dropout(0.25),\n        )\n        #input:(128, 64, 32, 32)\n        #output:(128, 128, 16, 16)\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), #(32-3+2)/1+1 = 32\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace = True),\n            \n            nn.MaxPool2d(kernel_size=2, stride=2, padding = 0), #(32-2+2*0)/2+1 = 16\n            nn.Dropout(0.3),\n        )\n        #input:(128, 128, 16, 16)\n        #output:(128, 256, 8, 8)\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1), #(16-3+2)/1+1 = 16\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace = True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding = 0), #(16-2+2*0)/2+1 = 8\n            nn.Dropout(0.35),\n        )\n        #input:(128, 256, 8, 8)\n        #output:(128, 512, 4, 4)\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1), #(8-3+2)/1+1 = 8\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace = True),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding = 0), #(8-2+2*0)/2+1 = 4\n            nn.Dropout(0.4),\n        )\n        self.conv1.apply(gaussian_weights_init)\n        self.conv2.apply(gaussian_weights_init)\n        self.conv3.apply(gaussian_weights_init)\n        self.conv4.apply(gaussian_weights_init)\n        #self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.2),\n            nn.Linear(512 * 4 * 4, 4096),\n            nn.RReLU(inplace = True),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, 1024),\n            nn.RReLU(inplace=True),\n            nn.Linear(1024, 256),\n            nn.RReLU(inplace=True),\n            nn.Linear(256, 7)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-13T15:35:48.161181Z","iopub.status.idle":"2021-11-13T15:35:48.162039Z","shell.execute_reply.started":"2021-11-13T15:35:48.161741Z","shell.execute_reply":"2021-11-13T15:35:48.161785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_loader, model, loss_fn, use_gpu=True):\n    model.train()\n    train_loss = []\n    train_acc = []\n    for (img, label) in train_loader:\n        if use_gpu:\n            img = img.to(device)\n            label = label.to(device)\n        optimizer.zero_grad()\n        output = model.forward(img)\n        loss = loss_fn(output, label)\n        loss.backward()            \n        optimizer.step()\n        with torch.no_grad():\n            predict = torch.argmax(output, dim=-1)\n            acc = np.mean((label == predict).cpu().numpy())\n            train_acc.append(acc)\n            train_loss.append(loss.item())\n            #train_loss.append(np.nan)\n    print(\"Epoch: {}, train Loss: {:.4f}, train Acc: {:.4f}\".format(epoch + 1, np.mean(train_loss), np.mean(train_acc)))\n    \ndef valid(valid_loader, model, loss_fn, use_gpu=True):\n    model.eval()\n    with torch.no_grad():\n        valid_loss = []\n        valid_acc = []\n        for idx, (img, label) in enumerate(valid_loader):\n            if use_gpu:\n                img = img.to(device)\n                label = label.to(device)\n            output = model(img)\n            loss = loss_fn(output, label)\n            predict = torch.argmax(output, dim=-1)\n            acc = (label == predict).cpu().tolist()\n            valid_loss.append(loss.item())\n            valid_acc += acc\n       \n        valid_acc = np.mean(valid_acc)\n        valid_loss = np.mean(valid_loss)\n        print(\"Epoch: {}, valid Loss: {:.4f}, valid Acc: {:.4f}\".format(epoch + 1, valid_loss, valid_acc))\n    return valid_acc\n\ndef save_checkpoint(valid_acc, acc_record, epoch, prefix='model'):\n#     you can define the condition to save model :)\n    if valid_acc >= np.mean(acc_record[-5:]):    \n        checkpoint_path = f'{prefix}.pth'\n        torch.save(model.state_dict(), checkpoint_path)\n        print('model saved to %s' % checkpoint_path)\n\ndef early_stop(valid_acc):\n    # TODO\n    if valid_acc >= 0.70:\n        return True\n    return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    model = FaceExpressionNet()\n    if use_gpu:\n        model.to(device)\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n    loss_fn = nn.CrossEntropyLoss()\n    \n    acc_record = []\n    \n    for epoch in range(NUM_ECPOCH):\n        train(train_loader, model, loss_fn, use_gpu)\n        valid_acc = valid(valid_loader, model, loss_fn, use_gpu=True)\n        acc_record.append(valid_acc)\n        \n        save_checkpoint(valid_acc, acc_record, epoch, prefix='model')\n        if early_stop(valid_acc):\n            break\n\n        \n        print('########################################################')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(test_loader, model, file_name='predict.csv'):\n    with torch.no_grad():\n        predict_result = []\n        for idx, img in enumerate(test_loader):\n            if use_gpu:\n                img = img.to(device)\n            output = model(img)\n            predict = torch.argmax(output, dim=-1).tolist()\n            predict_result += predict\n        \n    with open(file_name, 'w', newline='') as csvfile:\n        writer = csv.writer(csvfile)\n        writer.writerow(['id', 'label'])\n        for i in range(len(predict_result)):\n            writer.writerow([str(i), str(predict_result[i])])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T14:31:55.231543Z","iopub.execute_input":"2021-11-13T14:31:55.231938Z","iopub.status.idle":"2021-11-13T14:31:55.240057Z","shell.execute_reply.started":"2021-11-13T14:31:55.231901Z","shell.execute_reply":"2021-11-13T14:31:55.238401Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test(test_loader, model)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T13:53:32.117282Z","iopub.execute_input":"2021-11-11T13:53:32.117562Z","iopub.status.idle":"2021-11-11T13:54:06.246768Z","shell.execute_reply.started":"2021-11-11T13:53:32.117531Z","shell.execute_reply":"2021-11-11T13:54:06.246029Z"},"trusted":true},"execution_count":null,"outputs":[]}]}